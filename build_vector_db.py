import os
import json
import logging
from pathlib import Path
from typing import List, Dict
from datetime import datetime
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from tenacity import retry, stop_after_attempt, wait_exponential
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()

# ================== CONFIG ==================
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072  # text-embedding-3-large default dimension
BATCH_SIZE = 100
COLLECTION_NAME = "KHTN_QA"

# Paths
INPUT_JSON = "data/input/json/nhiet_hoc_VL-lop10-E.json"
DATABASE_FOLDER = "database"
QDRANT_PATH = f"{DATABASE_FOLDER}/qdrant_storage"
CHECKPOINT_FILE = f"{DATABASE_FOLDER}/embedding_checkpoint.json"

# Pricing (per 1M tokens)
EMBEDDING_COST_PER_1M = 0.13

# ================== LOGGING SETUP ==================
def setup_logging():
    """Setup logging"""
    log_folder = Path(DATABASE_FOLDER) / "logs"
    log_folder.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_folder / f"embedding_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# ================== CHECKPOINT MANAGEMENT ==================
def load_checkpoint() -> set:
    """Load danh s√°ch c√°c ID ƒë√£ embed"""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return set(data.get("embedded_ids", []))
    return set()

def save_checkpoint(embedded_ids: set):
    """Save checkpoint"""
    Path(DATABASE_FOLDER).mkdir(parents=True, exist_ok=True)
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump({
            "embedded_ids": list(embedded_ids),
            "last_updated": datetime.now().isoformat(),
            "total_embedded": len(embedded_ids)
        }, f, ensure_ascii=False, indent=2)

# ================== EMBEDDING WITH RETRY ==================
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    reraise=True
)
def get_embeddings_batch(client: OpenAI, texts: List[str]) -> List[List[float]]:
    """Embed m·ªôt batch texts v·ªõi retry logic"""
    response = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts
    )
    return [item.embedding for item in response.data]

# ================== MAIN PROCESS ==================
def main():
    logger.info("=" * 70)
    logger.info("B·∫ÆT ƒê·∫¶U QU√Å TR√åNH EMBEDDING V√Ä UPLOAD L√äN QDRANT")
    logger.info("=" * 70)
    
    # Check input JSON exists
    if not Path(INPUT_JSON).exists():
        logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {INPUT_JSON}")
        logger.error("   H√£y ch·∫°y script parse_questions_to_json.py tr∆∞·ªõc!")
        return
    
    # Load questions from JSON
    logger.info(f"üìÇ ƒê·ªçc c√¢u h·ªèi t·ª´: {INPUT_JSON}")
    with open(INPUT_JSON, 'r', encoding='utf-8') as f:
        all_questions = json.load(f)
    
    logger.info(f"üìä T·ªïng s·ªë c√¢u h·ªèi trong JSON: {len(all_questions)}")
    
    # Check for duplicate IDs - STOP if found
    logger.info("üîç Ki·ªÉm tra duplicate IDs...")
    ids = [q["id"] for q in all_questions]
    duplicate_ids = [id for id in ids if ids.count(id) > 1]
    
    if duplicate_ids:
        logger.error("=" * 70)
        logger.error("‚ùå PH√ÅT HI·ªÜN DUPLICATE IDs - D·ª™NG QU√Å TR√åNH")
        logger.error("=" * 70)
        logger.error(f"T√¨m th·∫•y {len(set(duplicate_ids))} ID tr√πng l·∫∑p:")
        
        for dup_id in sorted(set(duplicate_ids)):
            dup_questions = [q for q in all_questions if q["id"] == dup_id]
            logger.error(f"\nüî¥ ID: {dup_id} - Xu·∫•t hi·ªán {len(dup_questions)} l·∫ßn:")
            for i, q in enumerate(dup_questions, 1):
                logger.error(f"   [{i}] Question: {q['question'][:60]}...")
        
        logger.error("\nüí° Gi·∫£i ph√°p:")
        logger.error("   1. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu JSON")
        logger.error("   2. ƒê·∫£m b·∫£o m·ªói ID l√† duy nh·∫•t")
        raise ValueError(f"Found {len(set(duplicate_ids))} duplicate IDs. Fix them before embedding.")
    
    logger.info("‚úì Kh√¥ng c√≥ duplicate IDs")
    
    # Initialize clients
    logger.info("üîå Kh·ªüi t·∫°o OpenAI v√† Qdrant clients...")
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Initialize Qdrant (local mode)
    Path(QDRANT_PATH).mkdir(parents=True, exist_ok=True)
    qdrant_client = QdrantClient(path=QDRANT_PATH)
    logger.info(f"‚úì Qdrant client ƒë√£ k·∫øt n·ªëi t·ªõi: {QDRANT_PATH}")
    
    # Create collection if not exists
    collections = qdrant_client.get_collections().collections
    collection_exists = any(c.name == COLLECTION_NAME for c in collections)
    
    if not collection_exists:
        logger.info(f"üì¶ T·∫°o collection m·ªõi: {COLLECTION_NAME}")
        qdrant_client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(
                size=EMBEDDING_DIMENSIONS,
                distance=Distance.COSINE
            )
        )
        logger.info(f"‚úì ƒê√£ t·∫°o collection {COLLECTION_NAME}")
    else:
        logger.info(f"‚úì Collection {COLLECTION_NAME} ƒë√£ t·ªìn t·∫°i")
        collection_info = qdrant_client.get_collection(COLLECTION_NAME)
        logger.info(f"   S·ªë vectors hi·ªán c√≥: {collection_info.points_count}")
    
    # Load checkpoint
    embedded_ids = load_checkpoint()
    logger.info(f"üìã ƒê√£ embed tr∆∞·ªõc ƒë√≥: {len(embedded_ids)} c√¢u h·ªèi")
    
    # Filter out already embedded questions
    questions_to_embed = [q for q in all_questions if q["id"] not in embedded_ids]
    logger.info(f"üÜï C·∫ßn embed: {len(questions_to_embed)} c√¢u h·ªèi m·ªõi")
    
    if len(questions_to_embed) == 0:
        logger.info("‚úÖ T·∫•t c·∫£ c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c embed r·ªìi!")
        return
    
    # Estimate cost (bao g·ªìm c·∫£ explanation)
    total_chars = sum(
        len(q["question"]) + 
        len(q["correct_answer_text"]) + 
        len(q.get("explanation", ""))
        for q in questions_to_embed
    )
    total_tokens = total_chars * 0.75  # Rough estimate: 1 char ‚âà 0.75 tokens for Vietnamese
    
    estimated_cost = (total_tokens / 1_000_000) * EMBEDDING_COST_PER_1M
    logger.info(f"üí∞ ∆Ø·ªõc t√≠nh:")
    logger.info(f"   - T·ªïng k√Ω t·ª±: {total_chars:,}")
    logger.info(f"   - T·ªïng tokens (∆∞·ªõc): ~{total_tokens:,.0f}")
    logger.info(f"   - Chi ph√≠: ~${estimated_cost:.4f} (¬¢{estimated_cost*100:.2f})")
    
    # Confirm before proceeding
    logger.info("\n‚è∏Ô∏è  Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng, ho·∫∑c ch·ªù 3 gi√¢y ƒë·ªÉ ti·∫øp t·ª•c...")
    import time
    try:
        time.sleep(3)
    except KeyboardInterrupt:
        logger.info("\n‚ùå ƒê√£ h·ªßy b·ªüi ng∆∞·ªùi d√πng")
        return
    
    # Process in batches
    logger.info(f"\nüöÄ B·∫Øt ƒë·∫ßu embedding v·ªõi batch size = {BATCH_SIZE}")
    logger.info(f"üìù Embedding text format: question + correct_answer_text + explanation")
    
    total_batches = (len(questions_to_embed) + BATCH_SIZE - 1) // BATCH_SIZE
    points_to_upload = []
    
    for batch_idx in tqdm(range(0, len(questions_to_embed), BATCH_SIZE), 
                          desc="Embedding batches", 
                          total=total_batches):
        batch = questions_to_embed[batch_idx:batch_idx + BATCH_SIZE]
        
        # Prepare embedding texts (question + correct answer + explanation)
        embedding_texts = [
            f"{q['question']}\n" + 
            "\n".join([f"{k}. {v}" for k, v in q['options'].items()])
            for q in batch
        ]
        
        try:
            # Get embeddings
            embeddings = get_embeddings_batch(openai_client, embedding_texts)
            
            # Prepare points for Qdrant
            for question, embedding in zip(batch, embeddings):
                # Prepare payload - ch·ªâ l·∫•y c√°c tr∆∞·ªùng t·ªìn t·∫°i
                payload = {
                    "id": question["id"],
                    "question": question["question"],
                    "options": question["options"],
                    "correct_answer": question["correct_answer"],
                    "correct_answer_text": question["correct_answer_text"],
                    "explanation": question.get("explanation", ""),
                    "subject": question.get("subject", "unknown")
                }
                
                point = PointStruct(
                    id=hash(question["id"]) & 0x7FFFFFFFFFFFFFFF,  # Convert to positive int
                    vector=embedding,
                    payload=payload
                )
                points_to_upload.append(point)
                embedded_ids.add(question["id"])
            
            # Upload batch to Qdrant
            qdrant_client.upsert(
                collection_name=COLLECTION_NAME,
                points=points_to_upload[-len(batch):]
            )
            
            # Save checkpoint after each batch
            save_checkpoint(embedded_ids)
            
            logger.info(f"‚úì Batch {batch_idx//BATCH_SIZE + 1}/{total_batches}: "
                       f"ƒê√£ embed v√† upload {len(batch)} c√¢u h·ªèi")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω batch {batch_idx//BATCH_SIZE + 1}: {e}")
            logger.error("üõë D·ª™NG TO√ÄN B·ªò QU√Å TR√åNH DO L·ªñI EMBEDDING")
            raise
    
    # Final collection info
    collection_info = qdrant_client.get_collection(COLLECTION_NAME)
    
    # Final statistics
    logger.info("=" * 70)
    logger.info("K·∫æT QU·∫¢")
    logger.info("=" * 70)
    logger.info(f"‚úì T·ªïng s·ªë c√¢u h·ªèi ƒë√£ embed: {len(embedded_ids)}")
    logger.info(f"‚úì C√¢u h·ªèi m·ªõi embed trong l·∫ßn n√†y: {len(questions_to_embed)}")
    logger.info(f"‚úì Collection: {COLLECTION_NAME}")
    logger.info(f"‚úì S·ªë vectors trong Qdrant: {collection_info.points_count}")
    logger.info(f"‚úì Qdrant storage: {QDRANT_PATH}")
    logger.info(f"‚úì Checkpoint: {CHECKPOINT_FILE}")
    logger.info("=" * 70)
    logger.info("‚úÖ HO√ÄN T·∫§T!")

# ================== CLI ==================
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
    except Exception as e:
        logger.error(f"\n‚ùå L·ªói nghi√™m tr·ªçng: {e}", exc_info=True)