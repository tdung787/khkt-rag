"""
Module: VietnameseTextNormalizer (improved with batch processing)
Author: AI Assistant

Th√™m t√≠nh nƒÉng:
- X·ª≠ l√Ω c·∫£ folder (ƒë·ªçc t·∫•t c·∫£ file .txt)
- L∆∞u k·∫øt qu·∫£ v√†o JSON v·ªõi format: {"page": s·ªë_trang, "content": n·ªôi_dung}
- S·ªë trang ƒë∆∞·ª£c l·∫•y t·ª´ t√™n file (v√≠ d·ª•: bt10_text_5_0.txt -> page = 5)
"""

import re
import os
import json
from pathlib import Path
from typing import Optional, Dict, List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class VietnameseTextNormalizer:
    """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi regex v√† (tu·ª≥ ch·ªçn) LLM."""

    # B·∫£ng l·ªói/OCR ph·ªï bi·∫øn m·ªü r·ªông
    DEFAULT_TYPOS = {
        # OCR/simple typos
        'ph∆∞ong': 'ph∆∞∆°ng', 'ƒë√™u': 'ƒë·ªÅu', 'ƒëu·ª£c': 'ƒë∆∞·ª£c', 't√¢t': 't·∫•t',
        'm√¥t': 'm·ªôt', 'th∆∞c': 'th·ª±c', 'hi√™n': 'hi·ªán', 'tr∆∞∆°c': 'tr∆∞·ªõc',
        'b√¢t': 'b·∫•t', 'ng∆∞∆°i': 'ng∆∞·ªùi', 'ƒë∆∞∆°c': 'ƒë∆∞·ª£c', 'th√™m': 'th√™m',
        'n∆∞∆°c': 'n∆∞·ªõc', 'd∆∞∆°i': 'd∆∞·ªõi', 'tr∆∞∆°i': 'tr·ªùi', 'b∆∞∆°c': 'b∆∞·ªõc',
        'chu√¢n': 'chu·∫©n', 'ti√™n': 'ti·∫øn', 'nh√¢n': 'nh·∫≠n', 'g√¥m': 'g·ªìm',
        'ƒë∆∞∆°g': 'ƒë∆∞·ª£c', 'l∆∞∆°g': 'l∆∞·ª£ng', 'chi√™u': 'chi·ªÅu', 'th∆°i': 'th·ªùi',
        'lu√¢n': 'lu·∫≠n', 'tr√£i': 'tr·∫£i', 'tr√£i nghi·ªám': 'tr·∫£i nghi·ªám',
        'to√†n c·∫©u': 'to√†n c·∫ßu', 's·∫£n su·∫•t': 's·∫£n xu·∫•t', 'd√¢y chuy·ªÉn': 'd√¢y chuy·ªÅn',
        's·∫£n su·∫•t': 's·∫£n xu·∫•t', 'quan s√°t, tr√£i nghi·ªám': 'quan s√°t, tr·∫£i nghi·ªám',
        # common short forms
        '\bv\.?v\.?\b': 'v.v.', '\bvv\b': 'v.v.',
        # hyphenation issues
        'Ga-li-l√™': 'Ga-li-l√™',
    }

    # C√°c pattern ri√™ng ƒë·ªÉ x·ª≠ l√Ω (kh√¥ng n√™n ƒë∆∞a v√†o DEFAULT_TYPOS n·∫øu c·∫ßn regex)
    EXTRA_REPLACEMENTS = [
        # s·ª≠a spacing v√† d·∫•u c√¢u th∆∞·ªùng g·∫∑p
        (r'\s*,\s*', ', '),
        (r'\s*\.\s*', '. '),
        (r'\s*;\s*', '; '),
        (r'\s*:\s*', ': '),
        (r'\bA_\b', 'A.'),
        # S·ª¨A: Ch·ªâ thay A/B/C/D -> A./B./C./D. khi ·ªû ƒê·∫¶U D√íNG
        (r'^([A-D])\s+', r'\1. '),  # Th√™m ^ v√†o ƒë√¢y
        # s·ª≠a t·ª´ gh√©p hay ch·ªØ th∆∞·ªùng sau marker
        (r'^(?:[A-D]\.|\d+\.)\s*([a-z√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£≈©·ª±·ª≥·ªµ·ª∑])',
        lambda m: m.group(0)[0:-len(m.group(1))] + m.group(1).upper()),
    ]

    def __init__(self, use_llm: bool = False, openai_api_key: Optional[str] = None,
                 custom_typos: Optional[Dict[str, str]] = None):
        self.use_llm = use_llm
        self.typos = self.DEFAULT_TYPOS.copy()
        if custom_typos:
            self.typos.update(custom_typos)

        if use_llm:
            key = openai_api_key or os.getenv('OPENAI_API_KEY')
            if not key:
                raise ValueError("C·∫ßn OPENAI_API_KEY")
            self.client = OpenAI(api_key=key)

    def _extract_page_number(self, filename: str) -> int:
        """
        Tr√≠ch xu·∫•t s·ªë trang t·ª´ t√™n file.
        V√≠ d·ª•: 'page_021.png' -> 21
            'page_001.png' -> 1
            'bt10_text_5_0.txt' -> 5 (fallback cho format c≈©)
        """
        # Pattern t√¨m s·ªë sau 'page_'
        match = re.search(r'page_(\d+)', filename, re.IGNORECASE)
        if match:
            return int(match.group(1))
        
        # Fallback: Pattern t√¨m s·ªë sau '_text_' ho·∫∑c '_page_'
        match = re.search(r'_(?:text|page)_(\d+)', filename)
        if match:
            return int(match.group(1))
        
        # Fallback cu·ªëi: t√¨m s·ªë ƒë·∫ßu ti√™n trong t√™n file
        match = re.search(r'(\d+)', filename)
        if match:
            return int(match.group(1))
        
        return 0  # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y

    def _merge_broken_lines(self, lines: list) -> list:
        cleaned_lines = [line.replace('_', '').strip() for line in lines if line.strip()]
        merged = []
        i = 0
        while i < len(cleaned_lines):
            cur = cleaned_lines[i].strip()
            if not cur:
                i += 1
                continue
            if i + 1 < len(cleaned_lines):
                nxt = cleaned_lines[i + 1].strip()
                is_next_marker = (
                    re.match(r'^[A-D](?:[\.\s]|$)', nxt) or 
                    re.match(r'^\d+\.\s+\S', nxt)
                )
                ends_with_punct = re.search(r'[\.\?!:;]$', cur)
                starts_lowercase = nxt and nxt[0].islower()
                is_number_continuation = re.search(r'-$', cur) and re.match(r'^\d+\.$', nxt)
                if (not is_next_marker and (not ends_with_punct or starts_lowercase)) or is_number_continuation:
                    combined = cur + ' ' + nxt
                    merged.append(combined)
                    i += 2
                    continue
            merged.append(cur)
            i += 1
        return merged

    def _apply_typos(self, line: str) -> str:
        # √°p d·ª•ng c√°c thay th·∫ø ƒë∆°n gi·∫£n (bao g·ªìm c·∫£ regex keys trong typos)
        for typo, correct in self.typos.items():
            try:
                # n·∫øu typo l√† pattern regex (ch·ª©a \b ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát), d√πng re.sub
                if re.search(r'[^\w\s]', typo) or '\\b' in typo or '(' in typo:
                    line = re.sub(typo, correct, line, flags=re.IGNORECASE)
                else:
                    pattern = re.compile(r'\b' + re.escape(typo) + r'\b', re.IGNORECASE)
                    def repl(m):
                        text = m.group(0)
                        # gi·ªØ nguy√™n ki·ªÉu ch·ªØ ƒë·∫ßu
                        if text[0].isupper():
                            return correct.capitalize()
                        else:
                            return correct
                    line = pattern.sub(repl, line)
            except re.error:
                # fallback: literal replace
                line = line.replace(typo, correct)
        return line

    def _postprocess_punctuation(self, line: str) -> str:
        line = re.sub(r'\s+', ' ', line).strip()
        line = re.sub(r'\s*-\s*', ' - ', line)
        line = re.sub(r'\s+([,;:?!])', r'\1', line)
        line = re.sub(r'([,;:?!])(\S)', r'\1 \2', line)
        line = re.sub(r'([\.\?\!]){2,}', r'\1', line)
        line = re.sub(r'^([A-D])\s+', r'\1. ', line)
        if (
            not re.search(r'[\.\?!:;-]$', line) and
            len(line.split()) > 2 and
            not re.match(r'^[A-D]\.\s*\(\d+\)', line) and
            not re.match(r'^[A-D]\.$', line)
        ):
            line = line + '.'
        return line

    def normalize_with_regex(self, text: str) -> str:
        # B∆∞·ªõc 1: T√°ch d√≤ng khi g·∫∑p d·∫•u k·∫øt c√¢u + ƒë√°p √°n (A‚ÄìD)
        raw_text = re.sub(
            r'(?<=[.!?])\s+(?=[A-Da-d]\.)',
            '\n',
            text
        )

        # B∆∞·ªõc 2: Chu·∫©n h√≥a c√°c d√≤ng th√¥
        raw_lines = raw_text.split('\n')
        normalized_lines = []

        for ln in raw_lines:
            ln = ln.strip()
            if not ln:
                continue

            # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu ti√™n n·∫øu l√† a-d.
            ln = re.sub(r'^([a-d])(\.)', lambda m: f"{m.group(1).upper()}{m.group(2)}", ln)

            normalized_lines.append(ln)

        # B∆∞·ªõc 3: G·ªôp d√≤ng sau khi ƒë√£ chu·∫©n h√≥a ch·ªØ c√°i ƒë·∫ßu
        merged = self._merge_broken_lines(normalized_lines)

        # B∆∞·ªõc 4: L√†m s·∫°ch, thay th·∫ø l·ªói, gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng g·ªëc
        result_lines = []
        for ln in merged:
            ln = ln.strip()
            if not ln:
                continue

            # B·ªè g·∫°ch d∆∞·ªõi, th·ª´a kho·∫£ng tr·∫Øng
            ln = re.sub(r'_+', '', ln)
            ln = re.sub(r'\s{2,}', ' ', ln)

            # S·ª≠a l·ªói OCR
            ln = self._apply_typos(ln)

            # ƒê·∫£m b·∫£o format A. (kh√¥ng A . hay A_)
            ln = re.sub(r'^([A-D])\s*[_\.]?\s*', r'\1. ', ln)
            ln = re.sub(r'^(\d+\.)\s*', r'\1 ', ln)

            # Thay th·∫ø b·ªï sung
            for pat, repl in self.EXTRA_REPLACEMENTS:
                if callable(repl):
                    ln = re.sub(pat, repl, ln)
                else:
                    ln = re.sub(pat, repl, ln)

            # Gi·ªØ nguy√™n ch·ªØ th∆∞·ªùng sau d·∫•u ch·∫•m
            ln = self._postprocess_punctuation(ln)
            ln = re.sub(r'v\.v\.\.', 'v.v.', ln)

            result_lines.append(ln)

        return "\n".join(result_lines)


    def normalize_with_llm(self, text: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model="gpt-4.1-mini-2025-04-14",
                messages=[
                    {"role": "system", "content": "Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát (s·ª≠a l·ªói ch√≠nh t·∫£, d·∫•u c√¢u, kho·∫£ng tr·∫Øng). Gi·ªØ nguy√™n c·∫•u tr√∫c v√† ch·ªâ tr·∫£ vƒÉn b·∫£n ƒë√£ s·ª≠a."},
                    {"role": "user", "content": text}
                    
                ],
                temperature=0.2,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"L·ªói LLM: {e}")
            return text

        except Exception as e:
            print(f"L·ªói LLM: {e}")
            return text

    def normalize(self, text: str, method: str = 'regex') -> str:
        if method == 'regex':
            return self.normalize_with_regex(text)
        elif method == 'llm':
            if not self.use_llm:
                raise ValueError("Ch∆∞a b·∫≠t use_llm=True")
            return self.normalize_with_llm(text)
        elif method == 'hybrid':
            text = self.normalize_with_regex(text)
            if self.use_llm:
                text = self.normalize_with_llm(text)
            return text
        else:
            raise ValueError(f"Method kh√¥ng h·ª£p l·ªá: {method}")

    def normalize_file(self, input_path: str, output_path: str, method: str = 'regex'):
        """Chu·∫©n h√≥a 1 file ƒë∆°n l·∫ª"""
        with open(input_path, 'r', encoding='utf-8') as f:
            text = f.read()
        result = self.normalize(text, method=method)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(result)
        print(f"‚úì ƒê√£ l∆∞u: {output_path}")

    def normalize_folder(self, input_folder: str, output_json: str, output_txt_folder: Optional[str] = None, method: str = 'regex'):
        """
        Chu·∫©n h√≥a t·∫•t c·∫£ file .txt trong folder v√† l∆∞u v√†o JSON + c√°c file txt ri√™ng l·∫ª.
        
        Args:
            input_folder: ƒê∆∞·ªùng d·∫´n ƒë·∫øn folder ch·ª©a c√°c file .txt
            output_json: ƒê∆∞·ªùng d·∫´n file JSON output
            output_txt_folder: ƒê∆∞·ªùng d·∫´n folder ƒë·ªÉ l∆∞u c√°c file txt ƒë√£ chu·∫©n h√≥a (t√πy ch·ªçn)
            method: Ph∆∞∆°ng ph√°p chu·∫©n h√≥a ('regex', 'llm', ho·∫∑c 'hybrid')
        
        Output:
            - JSON file v·ªõi format: [{"page": 1, "content": "..."}, ...]
            - Folder ch·ª©a c√°c file .txt ri√™ng l·∫ª (n·∫øu output_txt_folder ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh)
        """
        input_path = Path(input_folder)
        if not input_path.exists():
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y folder: {input_folder}")
        
        # T·∫°o folder output cho txt files n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if output_txt_folder:
            output_txt_path = Path(output_txt_folder)
            output_txt_path.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ T·∫°o folder output: {output_txt_folder}")
        
        # L·∫•y t·∫•t c·∫£ file .txt
        txt_files = sorted(input_path.glob("*.txt"))
        if not txt_files:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .txt n√†o trong {input_folder}")
            return
        
        print(f"üìÇ T√¨m th·∫•y {len(txt_files)} file .txt")
        
        results = []
        
        for txt_file in txt_files:
            try:
                # ƒê·ªçc n·ªôi dung
                with open(txt_file, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                # Chu·∫©n h√≥a
                normalized_text = self.normalize(text, method=method)
                
                # Tr√≠ch xu·∫•t s·ªë trang t·ª´ t√™n file
                page_num = self._extract_page_number(txt_file.name)
                
                # Th√™m v√†o k·∫øt qu·∫£ JSON
                results.append({
                    "page": page_num,
                    "content": normalized_text,
                    "source_file": txt_file.name
                })
                
                # L∆∞u file txt ri√™ng l·∫ª (n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu)
                if output_txt_folder:
                    output_file = output_txt_path / txt_file.name
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(normalized_text)
                    print(f"‚úì X·ª≠ l√Ω: {txt_file.name} (trang {page_num}) ‚Üí {output_file.name}")
                else:
                    print(f"‚úì X·ª≠ l√Ω: {txt_file.name} (trang {page_num})")
                
            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω {txt_file.name}: {e}")
        
        # S·∫Øp x·∫øp theo s·ªë trang
        results.sort(key=lambda x: x['page'])
        
        # L∆∞u v√†o JSON
        output_json_path = Path(output_json)
        output_json_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ Ho√†n t·∫•t!")
        print(f"   - JSON: {output_json} ({len(results)} trang)")
        if output_txt_folder:
            print(f"   - TXT files: {output_txt_folder} ({len(txt_files)} files)")
        
        return output_json
