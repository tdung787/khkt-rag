import os
import json
import logging
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from tqdm import tqdm
from datetime import datetime

load_dotenv()

# ================== CONFIG ==================
MODEL = "gpt-4o-mini"
TEMPERATURE = 0  # ƒê·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
MIN_RESULT_LENGTH = 10  # ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa k·∫øt qu·∫£ h·ª£p l·ªá
MAX_RETRIES = 3  # S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa

SYSTEM_PROMPT = """\
B·∫°n l√† c√¥ng c·ª• g√°n ƒë√°p √°n ƒë√∫ng v√†o c√°c c√¢u h·ªèi tr·∫Øc nghi·ªám ti·∫øng Vi·ªát.

ƒê·∫ßu v√†o:
- "C√¢u h·ªèi": ch·ª©a nhi·ªÅu c√¢u h·ªèi tr·∫Øc nghi·ªám (C√¢u 1, C√¢u 2, ‚Ä¶), m·ªói c√¢u c√≥ c√°c l·ª±a ch·ªçn A. B. C. D.
- "ƒê√°p √°n": l√† ph·∫ßn vƒÉn b·∫£n ri√™ng, c√≥ th·ªÉ ch·ª©a m·ªôt ho·∫∑c nhi·ªÅu ƒë√°p √°n ƒë√∫ng, 
  kh√¥ng nh·∫•t thi·∫øt tr√πng s·ªë l∆∞·ª£ng ho·∫∑c th·ª© t·ª± v·ªõi c√¢u h·ªèi.

Y√™u c·∫ßu:
1. ƒê·ªçc hi·ªÉu n·ªôi dung c√°c c√¢u h·ªèi v√† ph·∫ßn ƒë√°p √°n.
2. V·ªõi m·ªói c√¢u h·ªèi, x√°c ƒë·ªãnh ƒë√°p √°n ƒë√∫ng (A, B, C, ho·∫∑c D) d·ª±a theo ng·ªØ nghƒ©a.
3. Ch√®n d√≤ng `<ƒê√°p √°n: X>` NGAY SAU d√≤ng l·ª±a ch·ªçn ƒë√∫ng ƒë√≥.
4. Gi·ªØ nguy√™n HO√ÄN TO√ÄN ƒë·ªãnh d·∫°ng g·ªëc c·ªßa c√¢u h·ªèi, kh√¥ng th√™m b·∫•t k·ª≥ k√Ω t·ª± markdown hay formatting n√†o.
5. N·∫øu kh√¥ng t√¨m th·∫•y ƒë√°p √°n ph√π h·ª£p, b·ªè tr·ªëng (kh√¥ng ch√®n g√¨).

V√ç D·ª§ 1 - ƒê√°p √°n l√† B:

C√¢u 1: C√¢u h·ªèi...
A. L·ª±a ch·ªçn A
B. L·ª±a ch·ªçn B (ƒë√¢y l√† ƒë√°p √°n ƒë√∫ng)
<ƒê√°p √°n: B>
C. L·ª±a ch·ªçn C
D. L·ª±a ch·ªçn D

V√ç D·ª§ 2 - ƒê√°p √°n l√† D:

C√¢u 2: C√¢u h·ªèi kh√°c...
A. L·ª±a ch·ªçn A
B. L·ª±a ch·ªçn B
C. L·ª±a ch·ªçn C
D. L·ª±a ch·ªçn D (ƒë√¢y l√† ƒë√°p √°n ƒë√∫ng)
<ƒê√°p √°n: D>

V√ç D·ª§ SAI - TUY·ªÜT ƒê·ªêI KH√îNG L√ÄM NH∆Ø V·∫¨Y:

C√¢u 3: ...
A. L·ª±a ch·ªçn A
<ƒê√°p √°n: B>  ‚Üê SAI! Kh√¥ng ƒë∆∞·ª£c ƒë·∫∑t tr∆∞·ªõc d√≤ng B
B. L·ª±a ch·ªçn B

L∆ØU √ù QUAN TR·ªåNG: 
- Tag <ƒê√°p √°n: X> ph·∫£i n·∫±m NGAY SAU d√≤ng l·ª±a ch·ªçn ƒë√∫ng (c√πng th·ª© t·ª± v·ªõi X)
- N·∫øu ƒë√°p √°n l√† C th√¨ <ƒê√°p √°n: C> ph·∫£i n·∫±m ngay sau d√≤ng "C. ..."
- N·∫øu ƒë√°p √°n l√† D th√¨ <ƒê√°p √°n: D> ph·∫£i n·∫±m ngay sau d√≤ng "D. ..."
- Tag <ƒê√°p √°n: X> ph·∫£i tr√™n m·ªôt d√≤ng ri√™ng, kh√¥ng n·ªëi li·ªÅn v·ªõi n·ªôi dung l·ª±a ch·ªçn
- KH√îNG th√™m b·∫•t k·ª≥ d·∫•u backtick (```), d·∫•u ngo·∫∑c, ho·∫∑c k√Ω t·ª± markdown n√†o
- Ch·ªâ output n·ªôi dung c√¢u h·ªèi v·ªõi tag <ƒê√°p √°n: X> ƒë∆∞·ª£c ch√®n v√†o, kh√¥ng c√≥ g√¨ kh√°c
"""

# ================== LOGGING SETUP ==================
def setup_logging(output_folder):
    """Thi·∫øt l·∫≠p logging v·ªõi file v√† console"""
    log_folder = Path(output_folder) / "logs"
    log_folder.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_folder / f"assignment_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# ================== API CALL WITH RETRY ==================
@retry(
    stop=stop_after_attempt(MAX_RETRIES),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((Exception,)),
    reraise=True
)
def call_openai_api(client, q_text, a_text, model=MODEL):
    """G·ªçi OpenAI API v·ªõi retry logic"""
    user_prompt = f"""\
D∆∞·ªõi ƒë√¢y l√† n·ªôi dung m·ªôt trang b√†i t·∫≠p tr·∫Øc nghi·ªám v√† ph·∫ßn ƒë√°p √°n t∆∞∆°ng ·ª©ng.
H√£y ƒë·ªçc hi·ªÉu v√† ch√®n ƒë√°p √°n ƒë√∫ng v√†o v·ªã tr√≠ ph√π h·ª£p theo ng·ªØ nghƒ©a.

--- C√ÇU H·ªéI ---
{q_text}

--- ƒê√ÅP √ÅN ---
{a_text}
"""
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        temperature=TEMPERATURE
    )
    
    result = response.choices[0].message.content.strip()
    
    # Validate k·∫øt qu·∫£
    if not result or len(result) < MIN_RESULT_LENGTH:
        raise ValueError(f"K·∫øt qu·∫£ t·ª´ API qu√° ng·∫Øn ho·∫∑c r·ªóng: {len(result)} k√Ω t·ª±")
    
    return result

# ================== SAFE FILE OPERATIONS ==================
def safe_read_file(file_path):
    """ƒê·ªçc file v·ªõi x·ª≠ l√Ω l·ªói encoding"""
    try:
        return Path(file_path).read_text(encoding="utf-8", errors='replace')
    except FileNotFoundError:
        raise
    except Exception as e:
        raise IOError(f"L·ªói ƒë·ªçc file {file_path}: {str(e)}")

def safe_write_file(file_path, content):
    """Ghi file v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        Path(file_path).write_text(content, encoding="utf-8", errors='replace')
        return True
    except Exception as e:
        raise IOError(f"L·ªói ghi file {file_path}: {str(e)}")

# ================== MAIN FUNCTION ==================
def assign_answers_with_ai(summary_json, questions_folder, answers_folder, output_folder, api_key=None):
    """
    G√°n ƒë√°p √°n v√†o t·ª´ng file c√¢u h·ªèi d·ª±a tr√™n AI (GPT-4o-mini) v·ªõi error handling n√¢ng cao
    """
    # Setup logging
    logger = setup_logging(output_folder)
    logger.info("=" * 60)
    logger.info("B·∫ÆT ƒê·∫¶U QU√Å TR√åNH G√ÅN ƒê√ÅP √ÅN B·∫∞NG AI")
    logger.info("=" * 60)
    
    # Initialize OpenAI client
    try:
        client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        logger.info(f"‚úì ƒê√£ k·∫øt n·ªëi OpenAI API v·ªõi model: {MODEL}")
    except Exception as e:
        logger.error(f"‚úó L·ªói kh·ªüi t·∫°o OpenAI client: {e}")
        return
    
    # Create output folder
    out_path = Path(output_folder)
    out_path.mkdir(parents=True, exist_ok=True)
    
    # Load summary
    try:
        with open(summary_json, "r", encoding="utf-8") as f:
            summary = json.load(f)
        logger.info(f"‚úì ƒê√£ load file summary: {summary_json}")
    except Exception as e:
        logger.error(f"‚úó L·ªói ƒë·ªçc file summary: {e}")
        return
    
    total = summary.get("total_files_processed", 0)
    
    # Tr√≠ch xu·∫•t danh s√°ch t·∫•t c·∫£ c√°c file t·ª´ c·∫•u tr√∫c JSON
    all_files = []
    for th_key, th_data in summary.items():
        if th_key.startswith("TH"):  # TH1, TH2, TH3, TH4
            logger.info(f"üìÇ {th_key}: {th_data.get('count', 0)} files")
            
            # L·∫•y file t·ª´ "bat_dau_Cau"
            if "bat_dau_Cau" in th_data:
                files = th_data["bat_dau_Cau"].get("files", [])
                all_files.extend(files)
                logger.info(f"   ‚îî‚îÄ bat_dau_Cau: {len(files)} files")
            
            # L·∫•y file t·ª´ "khong_bat_dau_Cau"
            if "khong_bat_dau_Cau" in th_data:
                files = th_data["khong_bat_dau_Cau"].get("files", [])
                all_files.extend(files)
                logger.info(f"   ‚îî‚îÄ khong_bat_dau_Cau: {len(files)} files")
    
    # Remove duplicates v√† sort
    all_files = sorted(set(all_files))
    
    logger.info(f"üìä T·ªïng s·ªë file c·∫ßn x·ª≠ l√Ω: {len(all_files)} (t·ª´ {total} file trong summary)")
    logger.info(f"   Sample files: {all_files[:3]}")
    
    if not all_files:
        logger.error("‚úó CRITICAL: Kh√¥ng t√¨m th·∫•y file n√†o ƒë·ªÉ x·ª≠ l√Ω!")
        return
    
    # Statistics
    stats = {
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "no_answer_file": 0
    }
    
    # Process each file with progress bar
    for fname in tqdm(all_files, desc="X·ª≠ l√Ω file", unit="file"):
        q_file = Path(questions_folder) / fname
        a_file = Path(answers_folder) / fname
        out_file = out_path / fname
        
        # Check question file exists
        if not q_file.exists():
            logger.warning(f"‚äò SKIP - Kh√¥ng t√¨m th·∫•y file c√¢u h·ªèi: {fname}")
            stats["skipped"] += 1
            continue
        
        # Check answer file exists
        if not a_file.exists():
            logger.warning(f"‚ö† WARNING - Kh√¥ng t√¨m th·∫•y file ƒë√°p √°n: {fname}")
            a_text = ""
            stats["no_answer_file"] += 1
        else:
            try:
                a_text = safe_read_file(a_file)
            except Exception as e:
                logger.error(f"‚úó ERROR - L·ªói ƒë·ªçc file ƒë√°p √°n {fname}: {e}")
                stats["failed"] += 1
                continue
        
        # Read question file
        try:
            q_text = safe_read_file(q_file)
        except Exception as e:
            logger.error(f"‚úó ERROR - L·ªói ƒë·ªçc file c√¢u h·ªèi {fname}: {e}")
            stats["failed"] += 1
            continue
        
        # Call AI API with retry
        try:
            result = call_openai_api(client, q_text, a_text)
            
            # Write result
            safe_write_file(out_file, result)
            logger.info(f"‚úì SUCCESS - {fname} ‚Üí ƒê√£ g√°n ƒë√°p √°n ({len(result)} k√Ω t·ª±)")
            stats["success"] += 1
            
        except ValueError as e:
            logger.error(f"‚úó VALIDATION ERROR - {fname}: {e}")
            stats["failed"] += 1
            
        except Exception as e:
            logger.error(f"‚úó ERROR - {fname}: {type(e).__name__} - {str(e)}")
            stats["failed"] += 1
    
    # Print final statistics
    logger.info("=" * 60)
    logger.info("K·∫æT QU·∫¢ T·ªîNG K·∫æT")
    logger.info("=" * 60)
    logger.info(f"‚úì Th√†nh c√¥ng: {stats['success']}/{len(all_files)}")
    logger.info(f"‚úó Th·∫•t b·∫°i: {stats['failed']}/{len(all_files)}")
    logger.info(f"‚äò B·ªè qua: {stats['skipped']}/{len(all_files)}")
    logger.info(f"‚ö† Kh√¥ng c√≥ file ƒë√°p √°n: {stats['no_answer_file']}/{len(all_files)}")
    logger.info(f"üìÅ Th∆∞ m·ª•c output: {output_folder}")
    logger.info("=" * 60)
    
    # Save statistics to JSON
    stats_file = out_path / "assignment_statistics.json"
    stats_data = {
        "timestamp": datetime.now().isoformat(),
        "model": MODEL,
        "temperature": TEMPERATURE,
        "total_files": len(all_files),
        "statistics": stats
    }
    
    try:
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úì ƒê√£ l∆∞u th·ªëng k√™ v√†o: {stats_file}")
    except Exception as e:
        logger.error(f"‚úó L·ªói l∆∞u file th·ªëng k√™: {e}")
    
    logger.info("\n‚úÖ HO√ÄN T·∫§T QU√Å TR√åNH G√ÅN ƒê√ÅP √ÅN")


# ================== CLI ==================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(
        description="G√°n ƒë√°p √°n tr·∫Øc nghi·ªám b·∫±ng AI v·ªõi error handling n√¢ng cao",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:
  python script.py \\
    --summary_json data/output/page_comparison_summary.json \\
    --questions_folder data/input/cleaned_text \\
    --answers_folder data/input/normalized_answers \\
    --output_folder data/output/ai_assigned
        """
    )
    parser.add_argument("--summary_json", required=True, 
                       help="ƒê∆∞·ªùng d·∫´n t·ªõi file JSON th·ªëng k√™")
    parser.add_argument("--questions_folder", required=True, 
                       help="Th∆∞ m·ª•c ch·ª©a file c√¢u h·ªèi")
    parser.add_argument("--answers_folder", required=True, 
                       help="Th∆∞ m·ª•c ch·ª©a file ƒë√°p √°n")
    parser.add_argument("--output_folder", required=True, 
                       help="Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£")
    parser.add_argument("--api_key", default=None, 
                       help="Tu·ª≥ ch·ªçn: API key OpenAI (n·∫øu kh√¥ng d√πng bi·∫øn m√¥i tr∆∞·ªùng)")
    
    args = parser.parse_args()

    assign_answers_with_ai(
        summary_json=args.summary_json,
        questions_folder=args.questions_folder,
        answers_folder=args.answers_folder,
        output_folder=args.output_folder,
        api_key=args.api_key
    )